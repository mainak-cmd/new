{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e1781c",
   "metadata": {},
   "source": [
    "### Frameworks and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63950a7",
   "metadata": {},
   "source": [
    "A library is a collection of pre-written code modules or functions that developers can use to perform specific tasks. Libraries are typically focused on providing specific functionalities or solving specific problems. Developers can selectively use the functions or modules from a library in their own code to add desired features or capabilities. Examples of libraries in Python include NumPy, pandas, and requests. Libraries offer flexibility as developers can choose which parts to use and integrate them into their codebase as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d9af4e",
   "metadata": {},
   "source": [
    "Frameworks typically include a set of libraries, tools, and conventions that help developers build applications in a specific domain or for a specific purpose. Frameworks often define the overall structure, design patterns, and flow of an application. Developers work within the framework's constraints and use its predefined components to build their applications. Examples of frameworks in Python include Django, Flask, and TensorFlow Frameworks provide a higher level of abstraction and can speed up development by providing a structured approach and handling common tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7b76d",
   "metadata": {},
   "source": [
    "Input Shape:\n",
    "\n",
    "For a Sequential model, the input shape is specified only for the first layer. Subsequent layers automatically infer the input shape from the previous layer's output.\n",
    "\n",
    "Convolutional Neural Networks (CNN):\n",
    "\n",
    "For CNNs, the input shape is typically a 3D tensor representing the image dimensions (height, width, channels).\n",
    "\n",
    "Recurrent Neural Networks (RNN):\n",
    "\n",
    "For RNNs, the input shape is a 3D tensor representing the sequence dimensions (batch size, timesteps, features).\n",
    "\n",
    "Output Shape:\n",
    "\n",
    "The output shape depends on the specific task and the desired output format. For example, in classification tasks, the output shape might be the number of classes for multi-class classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497504b0",
   "metadata": {},
   "source": [
    " ### multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49067ce1",
   "metadata": {},
   "source": [
    "A basic neural network, also known as a feedforward neural network or a multilayer perceptron (MLP),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685a9f7a",
   "metadata": {},
   "source": [
    "Input Layer:\n",
    "\n",
    "The input layer receives the input data for the neural network. Each neuron in the input layer represents a feature or input variable.\n",
    "\n",
    "The number of neurons in the input layer is determined by the number of input features or variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a120a33",
   "metadata": {},
   "source": [
    "Hidden Layer(s):\n",
    "\n",
    "Hidden layers are intermediate layers between the input and output layers. They perform computations on the input data and provide the network with the ability to learn complex patterns and representations.\n",
    "\n",
    "Each neuron in a hidden layer receives inputs from the previous layer and applies an activation function to produce an output.\n",
    "\n",
    "The number of hidden layers and the number of neurons in each hidden layer are configurable parameters of the network, depending on the complexity of the problem and the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfba9f31",
   "metadata": {},
   "source": [
    "Output Layer:\n",
    "\n",
    "The output layer produces the final output or prediction of the neural network.\n",
    "\n",
    "The number of neurons in the output layer depends on the nature of the problem. For example, for binary classification, a single neuron with a sigmoid activation function is often used. \n",
    "\n",
    "For multiclass classification, the number of neurons matches the number of classes, and a softmax activation function is typically applied.\n",
    "\n",
    "For regression problems, the output layer may have a single neuron without an activation function or with a linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75adf44",
   "metadata": {},
   "source": [
    "### Limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5af3a",
   "metadata": {},
   "source": [
    "Overfitting: MLPs are prone to overfitting, especially when dealing with complex and high-dimensional data. Overfitting occurs when the model learns to perform well on the training data but fails to generalize to unseen data. Regularization techniques such as dropout, L1/L2 regularization, or early stopping can help mitigate overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d7aab",
   "metadata": {},
   "source": [
    "Vanishing and Exploding Gradients: MLPs with many layers can suffer from vanishing or exploding gradients during the backpropagation process. \n",
    "\n",
    "Vanishing gradients lead to slow learning or non-learning, while exploding gradients can cause instability in training.\n",
    "\n",
    "Techniques like gradient clipping, proper weight initialization, or using activation functions like ReLU can alleviate these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a24ca58",
   "metadata": {},
   "source": [
    "Feature Engineering: MLPs often require careful feature engineering to extract meaningful representations from raw input data. \n",
    "This process involves preprocessing, scaling, and transforming the input features to enhance the network's ability to learn meaningful patterns.\n",
    "\n",
    "Insufficient feature engineering can lead to suboptimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979139a",
   "metadata": {},
   "source": [
    "Need for Sufficient Data: MLPs typically require a large amount of labeled training data to effectively learn complex patterns and generalize well. Insufficient data may lead to overfitting, poor performance, or difficulty in training a reliable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003f697",
   "metadata": {},
   "source": [
    "Model Selection and Hyperparameter Tuning: \n",
    "\n",
    "MLPs have various hyperparameters, such as the number of layers, number of neurons per layer, learning rate, activation functions, etc. Selecting the appropriate architecture and tuning the hyperparameters can be challenging and time-consuming.\n",
    "It often requires experimentation and domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc4b6e",
   "metadata": {},
   "source": [
    "Interpretability: MLPs can be considered as \"black-box\" models because they lack interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993bf9d8",
   "metadata": {},
   "source": [
    "Computational Complexity: As the number of layers and neurons in an MLP increases, the computational complexity of training and inference also increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7bdc27",
   "metadata": {},
   "source": [
    "### Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b778b",
   "metadata": {},
   "source": [
    "Non-linearity: MLPs are capable of learning complex non-linear relationships between inputs and outputs. This makes them effective in solving tasks that involve non-linear patterns or decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57e919",
   "metadata": {},
   "source": [
    "Universal Approximators: MLPs have been proven to be universal approximators, meaning they can approximate any continuous function to a desired level of accuracy given sufficient training data and appropriate network architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5815cdc3",
   "metadata": {},
   "source": [
    "Feature Learning: MLPs can automatically learn and extract relevant features from raw input data. This is particularly useful in scenarios where manual feature engineering is challenging or time-consuming. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b1c1d0",
   "metadata": {},
   "source": [
    "Scalability: MLPs can be scaled up to handle large and complex datasets. With the availability of computational resources, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610357ca",
   "metadata": {},
   "source": [
    "Parallelization: The training and inference processes in MLPs can be parallelized, leveraging the computational power of modern hardware, such as GPUs. This enables faster training and inference times, making MLPs suitable for real-time or time-sensitive applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39999b53",
   "metadata": {},
   "source": [
    "Transfer Learning: MLPs trained on one task can often be used as a starting point for related tasks. Transfer learning allows pre-trained MLP models to be fine-tuned or used as feature extractors, saving time and computational resources in training new models from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc6c3bd",
   "metadata": {},
   "source": [
    "Wide Range of Applications: MLPs have been successfully applied to various machine learning tasks, including classification, regression, pattern recognition, image and speech recognition, natural language processing, and more. Their versatility makes them a popular choice across different domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d892bf",
   "metadata": {},
   "source": [
    "Availability of Libraries and Tools: There are numerous open-source libraries and frameworks, such as TensorFlow, Keras, and PyTorch, that provide extensive support for building and training MLP models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44573e53",
   "metadata": {},
   "source": [
    "### USES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fc816",
   "metadata": {},
   "source": [
    "Classification\n",
    "\n",
    "Regression:\n",
    "    \n",
    "Pattern Recognition:\n",
    "    \n",
    "Natural Language Processing (NLP)\n",
    "\n",
    "Time Series Analysis:\n",
    "    \n",
    "Recommender Systems:\n",
    "    \n",
    "Control Systems:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73efcbef",
   "metadata": {},
   "source": [
    "## cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c37a5e8",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (CNN) is a type of neural network commonly used for image classification, object detection, and other computer vision tasks. CNNs are designed to automatically learn and extract relevant features from input images through a series of convolutional and pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8b6f92",
   "metadata": {},
   "source": [
    "Convolutional Layers: Convolutional layers are the primary building blocks of a CNN. They perform convolution operations on the input data using filters (also known as kernels). These filters slide across the input image, computing dot products with local regions and capturing spatial features such as edges, corners, and textures. The output of a convolutional layer is a feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29963df3",
   "metadata": {},
   "source": [
    "Pooling Layers: Pooling layers are used to reduce the spatial dimensions of the feature maps obtained from convolutional layers. The most common pooling operation is max pooling, where the maximum value within each local region is retained, discarding the rest. Pooling helps in reducing the computational complexity, extracting dominant features, and providing a form of translation invariance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f34552",
   "metadata": {},
   "source": [
    "Activation Function: An activation function is applied element-wise to the output of each neuron or convolutional operation. It introduces non-linearity into the network, enabling the model to learn complex relationships in the data. Common activation functions used in CNNs include ReLU (Rectified Linear Unit), sigmoid, and tanh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb21a0c",
   "metadata": {},
   "source": [
    "Fully Connected Layers: Fully connected layers, also known as dense layers, are traditional neural network layers where each neuron is connected to every neuron in the previous and next layers.\n",
    "    \n",
    "Fully connected layers are typically placed towards the end of the CNN and are responsible for making predictions or performing classification based on the features learned by the preceding layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c469824",
   "metadata": {},
   "source": [
    "Loss Function: The loss function defines the objective of the CNN and is used to measure the model's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ba7650",
   "metadata": {},
   "source": [
    "Optimization Algorithm: CNNs use optimization algorithms to adjust the weights and biases of the network during the training process. The goal is to minimize the loss function and improve the model's performance. Popular optimization algorithms include stochastic gradient descent (SGD), Adam, and RMSprop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48624037",
   "metadata": {},
   "source": [
    "Dropout: Dropout is a regularization technique commonly used in CNNs to prevent overfitting.\n",
    "\n",
    "It randomly sets a fraction of the neurons to zero during each training step, forcing the network to learn redundant representations and reducing the reliance on specific neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47239b75",
   "metadata": {},
   "source": [
    "### Limitation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae077ed",
   "metadata": {},
   "source": [
    "Difficulty with Capturing Long-Range Dependencies: CNNs are designed to capture local spatial patterns efficiently but may struggle to capture long-range dependencies in the data. \n",
    "    \n",
    "This limitation can be addressed by incorporating recurrent connections or using architectures like Transformer-based models, which are better suited for capturing global dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff1af4",
   "metadata": {},
   "source": [
    "Lack of Spatial Invariance: CNNs are sensitive to variations in spatial location. While this can be advantageous for tasks like object detection, it may hinder performance when there is a need for spatial invariance. Techniques like spatial pooling, data augmentation, and spatial transformer networks can help address this limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b8c8d",
   "metadata": {},
   "source": [
    "Difficulty with Small Datasets: CNNs typically require a large amount of labeled training data to generalize well. However, in cases where the dataset is small, techniques like transfer learning, fine-tuning, and data augmentation can be employed to leverage pre-trained models or artificially expand the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258a5ed",
   "metadata": {},
   "source": [
    "Interpretability and Explainability: CNNs are often referred to as \"black box\" models, as they can be challenging to interpret or explain their decision-making process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbabb87",
   "metadata": {},
   "source": [
    "Limited Applicability to Non-Grid Data: CNNs are primarily designed for grid-like input data, such as images. They may not be directly applicable to non-grid data, such as text or graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100d8899",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c5943c",
   "metadata": {},
   "source": [
    "Regularization techniques are used in machine learning to prevent overfitting, which occurs when a model performs well on the training data but fails to generalize to new, unseen data. Regularization helps in reducing the complexity of the model and controlling the model's ability to fit noise in the training data. Here are some commonly used regularization techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a12e7",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8ce162",
   "metadata": {},
   "source": [
    "In the context of machine learning and deep learning, an optimizer refers to an algorithm or method used to adjust the parameters of a model in order to minimize the error or loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e79dcc",
   "metadata": {},
   "source": [
    "Optimizers play a crucial role in training neural networks by iteratively updating the model's parameters based on the computed gradients of the loss function with respect to those parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d8160d",
   "metadata": {},
   "source": [
    "Gradient Descent: The basic form of optimization that iteratively adjusts the parameters in the direction of steepest descent of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ee21a",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD): An extension of gradient descent that performs the parameter updates using a randomly selected subset of training examples at each iteration, which can be computationally more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d68e5",
   "metadata": {},
   "source": [
    "RMSprop: A variant of gradient descent that uses an adaptive learning rate for each parameter based on the magnitude of recent gradients. It aims to mitigate the problem of diminishing learning rates in deep neural networks.\n",
    "\n",
    "Diminishing learning rates refer to a technique used in optimization algorithms to gradually reduce the step size or learning rate during the training process.\n",
    "\n",
    "In deep learning, diminishing learning rates can be beneficial for achieving better convergence and improving optimization performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d325355",
   "metadata": {},
   "source": [
    "Adagrad: An optimization algorithm that adapts the learning rate for each parameter based on the historical gradient information. It gives larger updates to infrequent parameters and smaller updates to frequent parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab815798",
   "metadata": {},
   "source": [
    "Adam: An adaptive optimization algorithm that combines ideas from both Adagrad and RMSprop. It maintains adaptive learning rates for each parameter and keeps an exponentially decaying average of past gradients and squared gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb14efb",
   "metadata": {},
   "source": [
    "AdaDelta: An extension of Adagrad that aims to address its aggressive, monotonically decreasing learning rate. AdaDelta adapts the learning rate based on a sliding window of past gradients instead of accumulating all past gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c66c4c",
   "metadata": {},
   "source": [
    "AdamW: A variant of Adam that incorporates weight decay regularization to mitigate overfitting. It decouples the weight decay from the learning rate schedule, which can improve optimization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe196e33",
   "metadata": {},
   "source": [
    "### Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72599bb4",
   "metadata": {},
   "source": [
    "An activation function, also known as a transfer function, is a mathematical function applied to the output of a neuron or a neural network layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b54c0",
   "metadata": {},
   "source": [
    "Introducing Non-linearity: Activation functions introduce non-linear transformations, allowing neural networks to model complex, non-linear relationships in the data. Without non-linear activation functions, a neural network would essentially be a linear model, limited to learning linear patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92badc31",
   "metadata": {},
   "source": [
    "Enabling Gradient Flow: During backpropagation, the gradients of the loss function with respect to the network's parameters are propagated backward through the network. Activation functions help in determining the gradients and enable the flow of these gradients, allowing the network to learn from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d7f9b",
   "metadata": {},
   "source": [
    "### Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b48f3",
   "metadata": {},
   "source": [
    "Sigmoid (Logistic) Function: The sigmoid function maps the input to a value between 0 and 1, which can be interpreted as a probability. It has a smooth, S-shaped curve and is often used in the output layer for binary classification problems. However, it is prone to vanishing gradients, which can make training slower and more challenging in deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef11b09",
   "metadata": {},
   "source": [
    "Hyperbolic Tangent (Tanh) Function: Similar to the sigmoid function, the tanh function squashes the input to a value between -1 and 1. It is symmetric around the origin and has steeper gradients compared to the sigmoid function. Tanh is commonly used in hidden layers of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee65c1",
   "metadata": {},
   "source": [
    "Rectified Linear Unit (ReLU): The ReLU function is a piecewise linear function that returns the input value if it is positive and zero otherwise. ReLU has become a popular activation function in deep learning due to its simplicity and effectiveness in mitigating the vanishing gradient problem. It allows for faster training and has been shown to work well in many applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac0352",
   "metadata": {},
   "source": [
    "Exponential Linear Unit (ELU): ELU is another variation of the ReLU function that provides negative values for negative inputs, allowing the activation to have a mean closer to zero. ELU has been reported to help with faster learning and better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a0d89f",
   "metadata": {},
   "source": [
    "Leaky ReLU: Leaky ReLU is a variation of the ReLU function that introduces a small slope for negative input values, allowing a small gradient to flow even for negative inputs. This helps alleviate the \"dying ReLU\" problem, where neurons can become permanently inactive during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ef224",
   "metadata": {},
   "source": [
    "Softmax : activation function commonly used in the output layer of a neural network, particularly for multiclass classification problems. It takes a vector of real-valued scores as input and produces a probability distribution over the classes.\n",
    "the softmax function exponentiates each score, making them positive, and then normalizes them by dividing by the sum of all exponentiated scores.\n",
    "\n",
    "The softmax function can be interpreted as a way to transform the output scores into probabilities. It amplifies large scores, making them more prominent in the resulting distribution, while suppressing small scores. This allows the model to assign higher probabilities to classes with higher scores, indicating higher confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26354e12",
   "metadata": {},
   "source": [
    "### lossfuntion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2976fc",
   "metadata": {},
   "source": [
    "Mean Squared Error (MSE) Loss: MSE is a regression loss function that calculates the average squared difference between the predicted and true values. It is commonly used in regression problems where the goal is to minimize the average squared deviation from the true values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c3a10",
   "metadata": {},
   "source": [
    "Binary Cross-Entropy Loss: Binary cross-entropy loss is used for binary classification problems where there are two possible classes. It measures the dissimilarity between the predicted probabilities and the true binary labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9234b80",
   "metadata": {},
   "source": [
    "Categorical Cross-Entropy Loss: Categorical cross-entropy loss is used for multiclass classification problems where there are more than two classes. It compares the predicted class probabilities with the true class labels and computes the average cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ba6f2",
   "metadata": {},
   "source": [
    "Sparse Categorical Cross-Entropy Loss: Similar to categorical cross-entropy, sparse categorical cross-entropy loss is used for multiclass classification problems with integer-encoded class labels. It avoids the need for one-hot encoding of the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be34871",
   "metadata": {},
   "source": [
    "Binary Hinge Loss: Binary hinge loss is used for binary classification tasks where the goal is to maximize the margin between positive and negative examples. It encourages correct classification while penalizing misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd2fef",
   "metadata": {},
   "source": [
    "Kullback-Leibler Divergence (KL Divergence) Loss: KL divergence is a measure of dissimilarity between two probability distributions. It is often used in tasks like generative modeling or when the model output needs to match a target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd920a51",
   "metadata": {},
   "source": [
    "Huber Loss: Huber loss combines characteristics of both mean absolute error (MAE) and mean squared error (MSE) loss functions. It is less sensitive to outliers than MSE and provides a more robust loss for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a672c4",
   "metadata": {},
   "source": [
    "Triplet Loss: Triplet loss is used in tasks like siamese networks or metric learning, where the goal is to learn embeddings or representations of samples such that similar samples are closer together and dissimilar samples are farther apart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a8681",
   "metadata": {},
   "source": [
    "### metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff94043",
   "metadata": {},
   "source": [
    "metrics used to evaluate the performance and effectiveness of AI models and systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88c40e",
   "metadata": {},
   "source": [
    "Accuracy: Accuracy measures how well a model predicts the correct output compared to the actual or expected output. It is often used in classification tasks to determine the percentage of correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c887f0",
   "metadata": {},
   "source": [
    "Precision and Recall: Precision and recall are metrics used in binary classification tasks. Precision measures the proportion of true positive predictions out of all positive predictions, while recall measures the proportion of true positive predictions out of all actual positive instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aa4cb0",
   "metadata": {},
   "source": [
    "F1 Score: The F1 score is a metric that combines precision and recall into a single value, providing a balanced measure of a model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561b324",
   "metadata": {},
   "source": [
    "Mean Squared Error (MSE): MSE is commonly used in regression tasks to measure the average squared difference between the predicted and actual values. It quantifies the overall quality of the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4510bfac",
   "metadata": {},
   "source": [
    "Mean Average Precision (mAP): mAP is often used in object detection tasks to evaluate the accuracy of bounding box predictions. It calculates the average precision for different levels of overlap between predicted and ground truth bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b515ce",
   "metadata": {},
   "source": [
    "Computational Efficiency: Metrics such as model size, inference time, and memory usage are used to assess the computational efficiency of AI models. These metrics are important for real-time and resource-constrained applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d88e5",
   "metadata": {},
   "source": [
    "Robustness and Generalization: Metrics like adversarial robustness, transfer learning performance, and cross-validation accuracy are used to evaluate the ability of AI models to generalize well to unseen data and handle variations and uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5235b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958080a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0f414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9475cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12135582",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mx_train\u001b[49m, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     18\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Define the MLP model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=784))  # Input layer with 784 input features\n",
    "model.add(Dense(units=64, activation='relu'))  # Hidden layer\n",
    "model.add(Dense(units=10, activation='softmax'))  # Output layer with 10 units for classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba7337c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18bac57e",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866dbbbc",
   "metadata": {},
   "source": [
    "Keras is a high-level deep learning API that is built on top of TensorFlow. It provides a user-friendly interface and simplifies the process of building, training, and deploying deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c3545a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected '(' (326356040.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def average:\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected '('\n"
     ]
    }
   ],
   "source": [
    "def average(a):\n",
    "    a=10\n",
    "    b=7\n",
    "    return (a+b)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c1a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avarage(a,b,c):\n",
    "    return (a+b+c)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f48df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage(2,4,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d53530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
